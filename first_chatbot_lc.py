# -*- coding: utf-8 -*-
"""First Chatbot- LC

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dbcYegvosnbrVMg4rA1NCQmoAVRlxTgv
"""

!pip install langchain==0.3.19 langchain-openai==0.3.7 langchain-community==0.3.18 langchain-text-splitters==0.3.6 faiss-cpu==1.10.0 beautifulsoup4==4.13.3 openai==1.64.0 python-dotenv==1.0.1

import os
os.environ["USER_AGENT"] = "MyLangChainApp/1.0"

from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# The client provides a URL [cite: 14]
loader = WebBaseLoader("https://github.com/NirDiamant/agents-towards-production/blob/main/tutorials/LangGraph-agent/langgraph_tutorial.ipynb")
data = loader.load()

# Split the input document as instructed
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
chunks = text_splitter.split_documents(data)

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
import os

# Hardcoding the API key as requested for this session
os.environ["OPENAI_API_KEY"] = "MYAPIKEY"

# 1. Initialize Embeddings (as suggested in Task 2)
embeddings = OpenAIEmbeddings()

# 2. Store chunks in FAISS Vector Store (as suggested in Task 3)
vector_store = FAISS.from_documents(chunks, embeddings)

print("Vector store created successfully!")

from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# Initialize Embeddings and Vector Store [cite: 23, 43, 46]
embeddings = OpenAIEmbeddings()
vector_store = FAISS.from_documents(chunks, embeddings)

llm = ChatOpenAI(model="gpt-3.5-turbo")

# This handles the "oldest iPhone" type of conversational context [cite: 59, 60]
contextualize_q_system_prompt = (
    "Given a chat history and the latest user question, "
    "formulate a standalone question which can be understood "
    "without the chat history."
)
contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", contextualize_q_system_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

retriever = create_history_aware_retriever(llm, vector_store.as_retriever(), contextualize_q_prompt)

# Define the final answering prompt [cite: 48, 50]
qa_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a virtual assistant for XYZ. Answer only from the provided context.\n\n{context}"),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

# Create the final chain
combine_docs_chain = create_stuff_documents_chain(llm, qa_prompt)
rag_chain = create_retrieval_chain(retriever, combine_docs_chain)

import gradio as gr

def chat_interface(message, history):
    # Format history for LangChain
    chat_history = []
    for human, ai in history:
        chat_history.append(("human", human))
        chat_history.append(("ai", ai))

    response = rag_chain.invoke({"input": message, "chat_history": chat_history})
    return response["answer"]

gr.ChatInterface(chat_interface).launch()